{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. 自定义Dataset类，处理不等长序列\n",
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, input_sequences):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.max_len = max(len(seq) for seq in input_sequences)  # 找到最长序列长度\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.input_sequences[idx]\n",
    "        # 填充序列至最大长度\n",
    "        padded_sequence = sequence + [[0] * len(sequence[0])] * (self.max_len - len(sequence))\n",
    "        return torch.tensor(padded_sequence, dtype=torch.float32), len(sequence)\n",
    "\n",
    "# 2. 定义一个Transformer模型来提取特征\n",
    "class MolecularEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads, num_layers):\n",
    "        super(MolecularEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)  # 特征嵌入层\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_dim,  # 输入维度\n",
    "            nhead=num_heads,        # 注意力头的数量\n",
    "            num_encoder_layers=num_layers  # Transformer层数\n",
    "        )\n",
    "        self.decoder = nn.Linear(embedding_dim, input_dim)  # 解码器，用于重建输入（自监督）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入嵌入到更高维度\n",
    "        embedded = self.embedding(x)\n",
    "        # Transformer的输入需要是(batch_size, seq_len, embedding_dim)，\n",
    "        # 但是transformer模型期望的输入是(seq_len, batch_size, embedding_dim)\n",
    "        embedded = embedded.permute(1, 0, 2)  # 转换成 (seq_len, batch_size, embedding_dim)\n",
    "        \n",
    "        # 使用Transformer模型\n",
    "        transformer_output = self.transformer(embedded)\n",
    "        \n",
    "        # 取Transformer输出的最后一层作为分子特征（或者你可以选择其他层的输出）\n",
    "        return transformer_output[-1, :, :]  # 取最后一时刻的特征\n",
    "\n",
    "# 3. 初始化数据和模型\n",
    "# 示例数据，假设每个分子有3个特征 (E1, E2, E3) + 持续时间特征\n",
    "input_sequences = [\n",
    "    [[1.0, 2.0, 3.0, 10], [2.0, 3.0, 4.0, 20]],  # 长度为2\n",
    "    [[2.0, 3.0, 4.0, 15], [3.0, 4.0, 5.0, 25], [4.0, 5.0, 6.0, 30]],  # 长度为3\n",
    "    [[1.5, 2.5, 3.5, 12]]  # 长度为1\n",
    "]\n",
    "\n",
    "# 初始化Dataset和DataLoader\n",
    "dataset = MolecularDataset(input_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = 4  # 假设每个分子有4个特征（3个化学成分 + 1个持续时间）\n",
    "embedding_dim = 128  # 特征嵌入维度\n",
    "num_heads = 4  # 自注意力头数量\n",
    "num_layers = 2  # Transformer层数\n",
    "model = MolecularEmbeddingModel(input_dim, embedding_dim, num_heads, num_layers)\n",
    "\n",
    "# 4. 模型前向传播并提取特征\n",
    "model.eval()  # 设置为评估模式\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (seq, seq_len) in enumerate(dataloader):\n",
    "        output = model(seq)\n",
    "        print(f\"Batch {batch_idx+1} Output: {output.shape}\")\n",
    "        # 输出每个序列的分子特征（每个分子的嵌入表示）\n",
    "        print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
